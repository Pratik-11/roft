{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CIS700 - Project TRICK Prompt Generation",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWeJ_SqyuPDw",
        "colab_type": "text"
      },
      "source": [
        "# CIS700 - Project \"TRICK\" Prompt Generation Script\n",
        "This is the notebook for generating prompts for the project [Learning To Trick Humans](https://github.com/kirubarajan/trick.git). \n",
        "\n",
        "Developed by Liam Dugan and Arun Kirubarajan in Spring 2020 for CIS700 \"Interactive Fiction and Text Generation\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZJ1zYjMvD-c",
        "colab_type": "text"
      },
      "source": [
        "## Step 1: Mount Drive and Clone the Repository"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9XpuJ-l7Z61H",
        "colab_type": "code",
        "outputId": "c963c4c4-5719-43a9-cdef-91ce7e811379",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# Mount your google drive folder\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Change to the google drive folder and clone our repo and gpt-2\n",
        "import os\n",
        "os.chdir('/content/drive/My Drive')\n",
        "!git clone https://github.com/kirubarajan/trick.git\n",
        "os.chdir('/content/drive/My Drive/trick')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "fatal: destination path 'trick' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCj3zNwFvOlo",
        "colab_type": "text"
      },
      "source": [
        "## Step 2: Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jA9aInplD0E_",
        "colab_type": "code",
        "outputId": "fc532ac0-9e6b-48b7-e0b4-4b77c4156d53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "%tensorflow_version 1.x      # GPT-2 currently only supports tensorflow 1 \n",
        "!pip3 install gpt-2-simple\n",
        "import nltk \n",
        "nltk.download('punkt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHCUuYPMvh5I",
        "colab_type": "text"
      },
      "source": [
        "## Step 3: Sample Prompts from Human Text\n",
        "At the moment this is only from AI dungeon training data, but we will expand this to include news and maybe blog posts later. \n",
        "\n",
        "### Usage\n",
        "1.   Determine desired NUM_GENERATIONS, MAX_PROMPT_LENGTH, and PERCENT_NONHUMAN\n",
        "2.   Pick the desired SAMPLE_FILE\n",
        "3.   That's it! Run and see!\n",
        "\n",
        "### Notes\n",
        "1.   Prompt lengths are sampled on a *uniform* distribution (not a random distribution)\n",
        "2.   Prompts are sampled from a file that is a combined train/dev/test file. This means that certain prompts may have been seen by the model in fine-tuning, we might want to change this in the future\n",
        "3.   Sampling file is read into memory dynamically as needed in order to save RAM. We read in prompt_length number of lines, however lines are not necessarily equal to sentences. When there are more sentences read in than lines this is no issue because we can just trim the excesses but the reverse case is a problem. We have yet to see it, but it can't be ruled out quite yet\n",
        "4.   We specifically re-roll any prompt that contains '<|startoftext|>' or '<|endoftext|>' for fear that it may bee too much of a giveaway. It is not sufficient to just remove these with a regex because the topic and setting of the text changes dramatically between appearances of these tokens. This does mean that the text that appears close to these tokens has a lower chance of appearing in a prompt.\n",
        "5.   We use nltk punkt sentence tokenizer to tokenize sentences. This isn't really a perfect tokenizer to use, might be worth looking into other options\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Aa-QESJsRjB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_file_lines(filename, start_line, stop_line):\n",
        "    output = []\n",
        "    with open(filename, 'r') as f:\n",
        "        for index, line in enumerate(f):\n",
        "          if index >= start_line and index < stop_line:\n",
        "            if line.isspace():\n",
        "              stop_line = stop_line + 1\n",
        "            else:\n",
        "              output.append(line)\n",
        "    return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PS9ZUMJYNfB-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "import re\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "NUM_GENERATIONS = 56 # Number of prompts to sample\n",
        "MAX_PROMPT_LENGTH = 11 # This is the maximum length of the prompt (lengths will be uniformly sampled from 1 to this number)\n",
        "PERCENT_NONHUMAN = 0.75 # This is the percentage of the prompts to have be randomly sampled from length 1 to MAX, the rest will all be human only\n",
        "SAMPLE_FILE = './samples/text_adventures.txt' # file to sample prompts from\n",
        "\n",
        "prompts = []\n",
        "for i in range(NUM_GENERATIONS):\n",
        "  # Randomly determine prompt length based on the specified percent nonhuman value\n",
        "  if (random.random() > PERCENT_NONHUMAN):\n",
        "    prompt_length = MAX_PROMPT_LENGTH\n",
        "  else:\n",
        "    prompt_length = random.randint(1, MAX_PROMPT_LENGTH - 1)\n",
        "\n",
        "  # Randomly determine which file to sample from (at the moment we're only sampling from AI dungeon becuase its easier)\n",
        "  # Eventually we should generalize and fix this up to sample from other domains\n",
        "  wc_output = !wc -l {SAMPLE_FILE}\n",
        "  file_len = wc_output[0].split()[0]\n",
        "\n",
        "  prompt_is_good = False\n",
        "  prompt_string = ''\n",
        "  while(not prompt_is_good):\n",
        "    # Randomly decide which line in the given file to sample from\n",
        "    starting_line = random.randint(1, int(file_len))\n",
        "    print('Starting Prompt at ' + str(starting_line))\n",
        "    print('Prompt Length is ' + str(prompt_length))\n",
        "\n",
        "    lines = read_file_lines(SAMPLE_FILE, starting_line, starting_line + prompt_length)\n",
        "    for line in lines:\n",
        "      prompt_string += line\n",
        "\n",
        "    prompt_is_good = True\n",
        "    if '<|startoftext|>' in prompt_string or '<|endoftext|>' in prompt_string:\n",
        "      prompt_is_good = False\n",
        "      prompt_string = ''\n",
        "\n",
        "  # Instead of going by line, decide prompt length on number of sentences in the prompt\n",
        "  cleaned_sentences = sent_tokenize(prompt_string)[:prompt_length]\n",
        "  prompts.append(cleaned_sentences)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEJb18g3tt27",
        "colab_type": "text"
      },
      "source": [
        "## Step 4: Download, Load, and Pre-Train GPT-2\n",
        "\n",
        "### Usage\n",
        "1.   Determine which GPT2_MODEL_NAME to use (sizes are in comments)\n",
        "2.   Determine number of FINETUNING_STEPS (1000 steps on 774M w/ colab pro took about an hour for reference)\n",
        "3.   Determine PRETRAINING_FILE_NAME to pretrain on\n",
        "4.   Run and see!\n",
        "\n",
        "### Notes\n",
        "1.   The gpt_2_simple library has a bad habit of not working if you ever interrupt it, so try your best to not interrupt it pretraining. If you do you will likely have to restart the runtime.\n",
        "2.   One good side effect of using gpt_2_simple is that it implicitly saves and loads checkpoints. This means if you get disconnected at any point, you can restart and as long as you have the same drive mounted to the same folder with the same parameters, it will find your most recently fine-tuned model. \n",
        "3.   One bad side effect of using gpt_2_simple is that it implicitly saves and loads checkpoints. This means if you ever want to fine tune a different model or switch around some of your parameters, the library has a tendency to assume you want to run from a checkpoint and error. You can fix these errors by manually going into your google drive and deleting the checkpoint\n",
        "4.   GPT-2 XL (1558M parameter) model is unable to fine tune using this library. I would love it if it were otherwise, but its the sad truth that Colab Pro's High-RAM GPUs still don't have enough VRAM. I get the feeling that there's an easy way around this and that it's becuase of a bug, but investigating that will be for another day. For the time being select XL at your own peril.\n",
        "5.   The number of finetuning steps being 1000 has no real reference. We should probably double check to see if that is actually a sufficient amount of fine tuning.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZqs89sj7kXc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gpt_2_simple as gpt2\n",
        "\n",
        "# Note trying to pretrain GPT-2 XL crashes even high-RAM colab pro\n",
        "GPT2_MODEL_NAME = \"774M\" # Small = 124M, Medium = 355M, Large = 774M, XL = 1558M\n",
        "FINETUNING_STEPS = 1000\n",
        "PRETRAINING_FILE_NAME = './samples/text_adventures_train.txt'\n",
        "\n",
        "if not os.path.isdir(os.path.join(\"models\", GPT2_MODEL_NAME)):\n",
        "\tprint(f\"Downloading {GPT2_MODEL_NAME} model...\")\n",
        "\tgpt2.download_gpt2(model_name=GPT2_MODEL_NAME)\n",
        "\n",
        "sess = gpt2.start_tf_sess()\n",
        "gpt2.finetune(sess, PRETRAINING_FILE_NAME, model_name=model_name, steps=FINETUNING_STEPS)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AP0infXDuK7b",
        "colab_type": "text"
      },
      "source": [
        "## Step 5: Generate Text with Fine-Tuned GPT-2 Model\n",
        "\n",
        "### Usage\n",
        "1.   Nothing special here, just run\n",
        "\n",
        "### Notes\n",
        "1.   We should probably split up the act of writing these generations to a JSON file every 20 or so generations. When I ran this for the first time, I crashed colab pro with OOM error at 94 generations and almost lost them all. This is probably the biggest TODO of the notebook in its current state.\n",
        "2.   Once again like the prompt sampling, we purposely re-roll the generation if GPT-2 ever gives us <|endoftext|> or <|startoftext|>. This does mean that we're arbitrarily skewing the distribution but hopefully that doesn't effect our results too much\n",
        "3.   The newlines in both the prompt and generated text are a bit of a nuisance. Currently we do not explicitly do anything with the newlines present in the prompt and generation because nltk tokenization generally takes care of them. However this means we do have to join the tokenized sentences with newlines when feeding gpt-2 the prompt. We may want to change this to space.\n",
        "4.   We keep newlines untouched in an attempt to match the fine-tuning dataset as closely as possible and with the assumption that sometimes newlines are meaningful. However, in an ideal world, we would prefer to have sentences that do not span 10+ turns of dialogue (which we have actually seen, believe it or not). Maybe in the future we could solve this by replacing all newlines in the fine-tuning dataset with spaces. I wonder how much damage this would cause.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69ukY84r8bXL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json5\n",
        "\n",
        "generations = []\n",
        "for index, prompt in enumerate(prompts):\n",
        "  if len(prompt) < MAX_PROMPT_LENGTH:\n",
        "    generation_is_good = False\n",
        "    while(not generation_is_good):\n",
        "      generated_text = gpt2.generate(sess, prefix='\\n'.join(prompt), return_as_list=True)[0]\n",
        "      generation_is_good = True\n",
        "      if '<|startoftext|>' in generated_text or '<|endoftext|>' in generated_text:\n",
        "        generation_is_good = False\n",
        "\n",
        "    ## Create the final output by concatenating the generation to the prompt with the separating boundary token\n",
        "    final_generation = prompt + sent_tokenize(generated_text)[len(prompt):MAX_PROMPT_LENGTH]\n",
        "    boundary = len(prompt) - 1\n",
        "  else:\n",
        "    final_generation = prompt\n",
        "    boundary = -1\n",
        "\n",
        "  generation = {\n",
        "      'prompt': final_generation[0],\n",
        "      'text': final_generation[1:],\n",
        "      'boundary': boundary,\n",
        "  }\n",
        "\n",
        "  print('=============GENERATION NUMBER: ' + str(index) + '=============')\n",
        "  print(generation)\n",
        "  generations.append(generation)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKdlhROL4nh1",
        "colab_type": "text"
      },
      "source": [
        "## Step 6: Output to JSON\n",
        "\n",
        "### Notes:\n",
        "1.   Using JSON5 here becuase it handles double quotes within JSON fields well, which is super important for dialogue based prompts like the ones we see in text_adventures\n",
        "2.   We concat the UNIX timestamp into the output file name to prevent overwriting. It's up to you to later go into your drive and combine however many of these JSON files you want together. \n",
        "3.   We need to give these prompts a unique ID, something to connect the annotation to them permanently, maybe a combination of the timestamp plus their prompt number ?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQeS4ASCBjVE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "25f3dcfc-8e2d-437e-c18b-7c85c06af4d9"
      },
      "source": [
        "import time\n",
        "output_file = './samples/generations_' + str(int(time.time())) + '.json'\n",
        "\n",
        "with open(output_file, 'w', encoding='utf-8') as f:\n",
        "    json5.dump(generations, f, ensure_ascii=False, indent=4)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "./samples/generations_1587592430.json\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}