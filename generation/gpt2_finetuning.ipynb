{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GPT-2 Fine Tuning v2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWeJ_SqyuPDw",
        "colab_type": "text"
      },
      "source": [
        "# Prompt Generation for ROFT (Real or Fake Text?) -- http://roft.io/\n",
        "\n",
        "Developed by Liam Dugan, Arun Kirubarajan, and Daphne Ippolito in Spring 2020 ([Github](https://github.com/kirubarajan/roft.git))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZJ1zYjMvD-c",
        "colab_type": "text"
      },
      "source": [
        "## Step 1: Mount Drive and Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9XpuJ-l7Z61H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Mount your google drive folder\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!gsutil cp gs://roft_datasets/story_prompts/reddit-stories-test.txt\n",
        "DATASET_NAME = 'reddit-stories-test'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6gk3S2T8dHOa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import random\n",
        "import mmap\n",
        "import numpy as np\n",
        "import json\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jA9aInplD0E_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk \n",
        "nltk.download('punkt')\n",
        "sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "\n",
        "!pip install -q git+https://github.com/huggingface/transformers.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHCUuYPMvh5I",
        "colab_type": "text"
      },
      "source": [
        "## Step 2: Sample Prompts from Human Text\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5KqQSiGyU0Jt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def print_prompt_sampled(prompt, index, total, line):\n",
        "  print('Sampled prompt {0}/{1} of length {2} from line {3}'.format(\n",
        "          str(index), str(total), str(len(prompt)), str(line)))\n",
        "  for line in prompt:\n",
        "    print('\\t' + repr(line))\n",
        "\n",
        "def print_prompt_too_short_warning(index, article_len, prompt_len):\n",
        "  print('Warning: Article #{0} (len: {1}) is too short for prompt length of {2}'.format(\n",
        "            str(index), str(article_len), str(prompt_len)))\n",
        "  \n",
        "def random_sample_prompt_len(percent_human, max_prompt_len):\n",
        "  if (random.random() < percent_human):\n",
        "    return max_prompt_len\n",
        "  else:\n",
        "    return random.randint(1, max_prompt_len)\n",
        "    \n",
        "def sample_corpus(sample_file, num_samples, max_prompt_len, percent_human, random_seed=436421):\n",
        "  random.seed(random_seed)\n",
        "\n",
        "  if not os.path.exists(sample_file):\n",
        "    print('Error: sample file \"' + sample_file + '\" does not exist')\n",
        "    exit(-1)\n",
        "\n",
        "  prompts = [] # The 2D array of prompts\n",
        "  num_shortened = 0 # The number of prompts that were too small to be full length\n",
        "  with open(sample_file, 'r+b') as f:\n",
        "    # mmap the file to avoid loading the whole thing into RAM\n",
        "    map_file = mmap.mmap(f.fileno(), 0, prot=mmap.PROT_READ)\n",
        "    \n",
        "    # Randomly decide which articles to grab our prompts from\n",
        "    wc_output = !wc -l $sample_file\n",
        "    num_lines = int(wc_output[0].split()[0])\n",
        "    articles_to_sample = random.sample(range(num_lines), num_samples)\n",
        "\n",
        "    # Iterate over all articles in the file and sample from only the selected articles\n",
        "    for index, line in enumerate(iter(map_file.readline, b\"\")):\n",
        "      if index not in articles_to_sample: continue\n",
        "\n",
        "      # Randomly determine this prompt's length based on the percent human value\n",
        "      prompt_length = random_sample_prompt_len(percent_human, max_prompt_len)\n",
        "\n",
        "      # Use NLTK Sentence tokenizer to split this prompt into sentences\n",
        "      article = sent_detector.tokenize(str(line, 'utf-8', 'ignore'))\n",
        "\n",
        "      # If article is shorter than the desired prompt length, shorten the prompt length\n",
        "      if len(article) < prompt_length: \n",
        "        print_prompt_too_short_warning(index, len(article), prompt_length)\n",
        "        prompt_length = len(article)\n",
        "        num_shortened += 1\n",
        "\n",
        "      # Append the prompt to the list of prompts\n",
        "      prompts.append(article[:prompt_length])\n",
        "\n",
        "      print_prompt_sampled(article[:prompt_length], len(prompts), num_samples, index)\n",
        "  \n",
        "  print('Warning: {0} articles were too short for prompt length'.format(str(num_shortened)))\n",
        "  return prompts\n",
        "\n",
        "NUM_EXAMPLES_PER_HPARAM = 100\n",
        "HPARAM_VALUES = np.arange(0.000000, 1.10000000, 0.10000000).tolist()\n",
        "NUM_GENS = NUM_EXAMPLES_PER_HPARAM * len(HPARAM_VALUES)\n",
        "MAX_LEN = 10\n",
        "PERCENT_HUMAN = 0.20\n",
        "\n",
        "prompts = sample_corpus('/content/reddit-stories-test.txt', NUM_GENS, MAX_LEN, PERCENT_HUMAN)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q76vHFE1VDq4",
        "colab_type": "text"
      },
      "source": [
        "## Step 3: Filter Prompts for Duplicates and Profanity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFE2SIU8VJaw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip3 install profanity_filter\n",
        "!python3 -m spacy download en\n",
        "from profanity_filter import ProfanityFilter\n",
        "pf = ProfanityFilter()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ejAnX_oVMhQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def filter_profanity(prompts):\n",
        "  return [prompt for prompt in prompts if not pf.is_profane(''.join(prompt))]\n",
        "\n",
        "def filter_duplicates(prompts): \n",
        "  ''' For our purposes, a duplicate is considered to be two prompts with same starting sent\n",
        "     Because we only ever sample from beginning of articles, this means that this function \n",
        "     will filter out all prompts from the same article even if they have different lengths '''\n",
        "  prompt_set = set([prompt[0] for prompt in prompts]) # Use a set to avoid O(n^2)\n",
        "  for prompt in prompts:\n",
        "    prompt_set.discard(prompt[0])\n",
        "    if prompt[0] in prompt_set:\n",
        "      prompts.remove(prompt)\n",
        "  return prompts\n",
        "\n",
        "pre_filter_len = len(prompts)\n",
        "prompts = filter_profanity(prompts)\n",
        "print(\"Profanity filtering reduced len by \" + str(pre_filter_len - len(prompts)))\n",
        "\n",
        "pre_filter_len = len(prompts)\n",
        "prompts = filter_duplicates(prompts)\n",
        "print(\"Duplicate filtering reduced len by \" + str(pre_filter_len - len(prompts)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEJb18g3tt27",
        "colab_type": "text"
      },
      "source": [
        "## Step 4: Generate with pre-trained GPT-2\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1cjxr1VeWW5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "prompts_per_p = {}\n",
        "for idx, p in enumerate(HPARAM_VALUES):\n",
        "  prompts_per_p[p] = prompts[idx*NUM_EXAMPLES_PER_HPARAM:(idx+1)*NUM_EXAMPLES_PER_HPARAM]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZqs89sj7kXc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from transformers import TFGPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = TFGPT2LMHeadModel.from_pretrained(\"gpt2-xl\", pad_token_id=tokenizer.eos_token_id)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCcHIlLXim-k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate(prompts_per_p, p):\n",
        "  # set seed to reproduce results. Feel free to change the seed though to get different results\n",
        "  tf.random.set_seed(int(p*100))\n",
        "\n",
        "  examples = []\n",
        "\n",
        "  for prompt_index, prompt in enumerate(prompts_per_p[p]):\n",
        "    print('{}/{}'.format(prompt_index+1, len(prompts_per_p[p])))\n",
        "    if len(prompt) == 10:\n",
        "      examples.append({\n",
        "          'prompt': prompt,\n",
        "          'continuation': []\n",
        "      })\n",
        "    else:\n",
        "      prompt_text = ' '.join(prompt)\n",
        "      input_ids = tokenizer.encode(prompt_text, return_tensors='tf')\n",
        "\n",
        "      continuation_length = 0\n",
        "      retry_counter = 0\n",
        "\n",
        "      while continuation_length + len(prompt) < 10 and retry_counter < 5:\n",
        "        retry_counter += 1\n",
        "\n",
        "        # deactivate top_k sampling and sample only from 92% most likely words\n",
        "        start_time = time.time()\n",
        "        sample_output = model.generate(\n",
        "            input_ids, \n",
        "            do_sample=True, \n",
        "            max_length=256, \n",
        "            top_p=p, \n",
        "            top_k=0\n",
        "        )\n",
        "        total_time = time.time() - start_time\n",
        "\n",
        "        output_tokens = sample_output[0]\n",
        "        continuation = tokenizer.decode(output_tokens, skip_special_tokens=True)\n",
        "        continuation = sent_detector.tokenize(continuation)[len(prompt):]\n",
        "        continuation_length = len(continuation)\n",
        "        print('{} sentences in prompt, {} sentences generated in {} seconds'.format(\n",
        "            len(prompt), continuation_length, total_time))\n",
        "        print('Total length is {}'.format(continuation_length + len(prompt)))\n",
        "\n",
        "        continuation = continuation[:(10 - len(prompt))]\n",
        "\n",
        "      examples.append({\n",
        "          'prompt': prompt,\n",
        "          'continuation': continuation\n",
        "      })\n",
        "    to_save = {\n",
        "        'dataset': 'reddit-stories',\n",
        "        'split': 'test',\n",
        "        'p': p,\n",
        "        'examples': examples\n",
        "    }\n",
        "  with open('examples-{}-p{}.json'.format(DATASET_NAME, p), 'w') as f:\n",
        "    json.dump(to_save, f, indent=2)\n",
        "  \n",
        "  return to_save\n",
        "\n",
        "generate(prompts_per_p, 0.0)\n",
        "#!gsutil cp /content/examples-reddit-stories-test-p0.0.json  gs://roft_datasets/generations/"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}