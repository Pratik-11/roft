{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GPT-2 Fine Tuning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWeJ_SqyuPDw",
        "colab_type": "text"
      },
      "source": [
        "# Prompt Generation for ROFT (Real or Fake Text?) -- http://roft.io/\n",
        "\n",
        "Developed by Liam Dugan and Arun Kirubarajan in Spring 2020 ([Github](https://github.com/kirubarajan/roft.git))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZJ1zYjMvD-c",
        "colab_type": "text"
      },
      "source": [
        "## Step 1: Mount Drive and Clone the Repository"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9XpuJ-l7Z61H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Mount your google drive folder\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Change to the google drive folder and clone our repo and gpt-2\n",
        "import os\n",
        "os.chdir('/content/drive/My Drive')\n",
        "!git clone https://github.com/kirubarajan/roft.git\n",
        "os.chdir('/content/drive/My Drive/roft')\n",
        "!git fetch\n",
        "!git pull"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCj3zNwFvOlo",
        "colab_type": "text"
      },
      "source": [
        "## Step 2: Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jA9aInplD0E_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 1.x      # GPT-2 currently only supports tensorflow 1 \n",
        "\n",
        "# Make sure you're running on the GPU\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if not 'GPU' in device_name:\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))\n",
        "\n",
        "!pip3 install gpt-2-simple\n",
        "import nltk \n",
        "nltk.download('punkt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHCUuYPMvh5I",
        "colab_type": "text"
      },
      "source": [
        "## Step 3: Sample Prompts from Human Text\n",
        "At the moment this is only from AI dungeon training data, but we will expand this to include news and maybe blog posts later. \n",
        "\n",
        "### Usage\n",
        "1.   Determine desired NUM_GENS, MAX_PROMPT_LEN, and PERCENT_HUMAN\n",
        "2.   Pick the desired SAMPLE_FILE\n",
        "3.   That's it! Run and see!\n",
        "\n",
        "### Notes\n",
        "1.   Prompt lengths are sampled on a *uniform* distribution (not a random distribution)\n",
        "2.   Prompts are only sampled from the AI Dungeon *test* file. We should probably concatenate both the test and dev file to get maximum prompt diversity.\n",
        "3.   The file is mmaped instead of loading it into RAM, so no need to worry about having a high-RAM instance of colab pro.\n",
        "4.   We only sample from the start of generations (<|startoftext|>). This allows us to avoid instances where a prompt contains context from a previous unseen part of the story\n",
        "5.   We use nltk punkt sentence tokenizer to tokenize sentences. This isn't really a perfect tokenizer to use, might be worth looking into other options\n",
        "6.   There ARE duplicates. This is because the input text_adventures_test.txt has duplicates. Will probably MD5 hash check at some point in the future to get rid of these\n",
        "7.   Prompts are always sampled from the beginning of the file and not randomly. This is to avoid running the regex on the entire file for large corpora. This can be changed in the future, but for now, do not expect this to give you two different sets of generations if run twice.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URFBnYZtbISX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os, re, mmap, random\n",
        "from nltk.tokenize import sent_tokenize"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4iRynfvRR-0S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def random_sample_prompt_len(percent_human, max_prompt_len):\n",
        "  if (random.random() < percent_human):\n",
        "    return max_prompt_len\n",
        "  else:\n",
        "    return random.randint(1, max_prompt_len - 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJub8y5VGmVc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sample_ai_dungeon(num_samples, max_prompt_len, percent_human):\n",
        "\n",
        "  # AI Dungeon sample file is text_adventures_test.txt\n",
        "  # (should probably update to include dev at some point)\n",
        "  sample_file = './generation/AI-Dungeon/text_adventures_test.txt'\n",
        "  if not os.path.exists(sample_file):\n",
        "    print('Error: AI Dungeon sample file \"' + sample_file + '\" does not exist')\n",
        "    exit(-1)\n",
        "\n",
        "  prompts = []\n",
        "  successfully_sampled_prompts = 0\n",
        "  with open(sample_file, 'r+b') as f:\n",
        "    # mmap the file so we can regex it without loading it all into RAM\n",
        "    data = mmap.mmap(f.fileno(), 0)\n",
        "\n",
        "    # Grab all the spans of text that are between <|startoftext|> and <|endoftext|>\n",
        "    # (use finditer instead of findall to only search for regex matches as necessary)\n",
        "    pattern = re.compile(b'<\\|startoftext\\|\\>((.|\\n)*?)\\<\\|endoftext\\|\\>')\n",
        "    for m in re.finditer(pattern, data):\n",
        "      # If we're done sampling, no need to continue the loop\n",
        "      if successfully_sampled_prompts >= num_samples: break\n",
        "\n",
        "      # Randomly determine prompt length based on the specified percent nonhuman value\n",
        "      prompt_length = random_sample_prompt_len(percent_human, max_prompt_len)\n",
        "\n",
        "      # Use NLTK Sentence tokenizer to sample sentences and clean them\n",
        "      tokenized_prompt = sent_tokenize(str(m.group(1), 'utf-8', 'ignore'))\n",
        "\n",
        "      # Accept the prompt if it is longer than the desired length\n",
        "      if len(tokenized_prompt) > prompt_length:\n",
        "        prompts.append(tokenized_prompt[:prompt_length])\n",
        "        successfully_sampled_prompts += 1\n",
        "\n",
        "        print('Sampled prompt {0} of length {1}'.format(\n",
        "            str(successfully_sampled_prompts), str(prompt_length)))\n",
        "        for line in tokenized_prompt[:prompt_length]:\n",
        "          print('\\t' + repr(line))\n",
        "\n",
        "  return prompts"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a03329XaBLYs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def print_prompt_sampled(prompt, index, total, line):\n",
        "\n",
        "  print('Sampled prompt {0}/{1} of length {2} from line {3}'.format(\n",
        "          str(index), str(total), str(len(prompt)), str(line)))\n",
        "  \n",
        "  for line in prompt:\n",
        "    print('\\t' + repr(line))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VlbVLn3yS06W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sample_nyt(num_samples, max_prompt_len, percent_human):\n",
        "\n",
        "  # NYT sample file is nyt-articles.txt, if this file does not exist, import a selection of\n",
        "  # .ta.xml files from the NYT corpus into a folder named /raw and run process-NYT.py\n",
        "  # This will create a train/test split and output articles, one per line, into the sample file\n",
        "  sample_file = './generation/New-York-Times/nyt-articles-test.txt'\n",
        "  if not os.path.exists(sample_file):\n",
        "    print('Error: NYT sample file \"' + sample_file + '\" does not exist')\n",
        "    exit(-1)\n",
        "      \n",
        "  # Count the number of lines of the given sample file so we can randomly sample later\n",
        "  wc_output = !wc -l $sample_file\n",
        "  num_lines = int(wc_output[0].split()[0])\n",
        "\n",
        "  prompts = []\n",
        "  num_shortened = 0\n",
        "  with open(sample_file, 'r+b') as f:\n",
        "    # mmap the file to avoid loading the whole thing into RAM\n",
        "    map_file = mmap.mmap(f.fileno(), 0, prot=mmap.PROT_READ)\n",
        "    \n",
        "    # Randomly sample which articles to grab our prompts from\n",
        "    # Here we select the line numbers b/c articles are processed to be one per line\n",
        "    articles_to_sample = random.sample(range(num_lines), num_samples)\n",
        "\n",
        "    # Iterate over all articles in the file and sample from the selected articles\n",
        "    for index, line in enumerate(iter(map_file.readline, b\"\")):\n",
        "      # If the article wasn't selected for sampling, don't sample\n",
        "      if index not in articles_to_sample: continue\n",
        "\n",
        "      # Randomly determine prompt length based on the percent nonhuman value\n",
        "      prompt_length = random_sample_prompt_len(percent_human, max_prompt_len)\n",
        "\n",
        "      # Use NLTK Sentence tokenizer to sample sentences and clean them\n",
        "      article = sent_tokenize(str(line, 'utf-8', 'ignore'))\n",
        "\n",
        "      # If article is shorter than the desired prompt length, shorten the prompt length\n",
        "      if len(article) < prompt_length: \n",
        "        print('Warning: Article #{0} (len: {1}) is too short for prompt length of {2}'.format(\n",
        "            str(index), str(len(article)), str(prompt_length)))\n",
        "        prompt_length = len(article)\n",
        "        num_shortened += 1\n",
        "\n",
        "      # Add the prompt to the list of prompts\n",
        "      prompts.append(article[:prompt_length])\n",
        "\n",
        "      print_prompt_sampled(article[:prompt_length], len(prompts), num_samples, index)\n",
        "  \n",
        "  print('Warning: {0} articles were too short for prompt length'.format(str(num_shortened)))\n",
        "\n",
        "  return prompts"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PS9ZUMJYNfB-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "import re\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "NUM_GENS = 50 # Number of prompts to sample\n",
        "MAX_PROMPT_LEN = 11 # Max len of prompt (uniformly sampled from 1 to this)\n",
        "PERCENT_HUMAN = 0.25 # %age of prompts to have be all human\n",
        "SAMPLING_CORPUS = 'NYT' # Name of corpus to sample from (\"NYT\", \"Reddit\", \"Dungeon\")\n",
        "\n",
        "if SAMPLING_CORPUS == 'Dungeon':\n",
        "  prompts = sample_ai_dungeon(NUM_GENS, MAX_PROMPT_LEN, PERCENT_HUMAN)\n",
        "elif SAMPLING_CORPUS == 'NYT':\n",
        "  prompts = sample_nyt(NUM_GENS, MAX_PROMPT_LEN, PERCENT_HUMAN)\n",
        "elif SAMPLING_CORPUS == 'Reddit':\n",
        "  prompts = sample_reddit(NUM_GENS, MAX_PROMPT_LEN, PERCENT_HUMAN)\n",
        "else:\n",
        "  print('Error: Invalid sampling corpus name ' + SAMPLING_CORPUS)\n",
        "  exit(-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEJb18g3tt27",
        "colab_type": "text"
      },
      "source": [
        "## Step 4: Download, Load, and Fine-Tune GPT-2\n",
        "\n",
        "### Usage\n",
        "1.   Determine which GPT2_MODEL_NAME to use (sizes are in comments)\n",
        "2.   Determine number of FINETUNING_STEPS (1000 steps on 774M w/ colab pro took about an hour for reference)\n",
        "3.   Determine PRETRAINING_FILE_NAME to pretrain on\n",
        "4.   Run and see!\n",
        "\n",
        "### Notes\n",
        "1.   The gpt_2_simple library has a bad habit of not working if you ever interrupt it, so try your best to not interrupt it pretraining. If you do you will likely have to restart the runtime.\n",
        "2.   One good side effect of using gpt_2_simple is that it implicitly saves and loads checkpoints. This means if you get disconnected at any point, you can restart and as long as you have the same drive mounted to the same folder with the same parameters, it will find your most recently fine-tuned model. \n",
        "3.   One bad side effect of using gpt_2_simple is that it implicitly saves and loads checkpoints. This means if you ever want to fine tune a different model or switch around some of your parameters, the library has a tendency to assume you want to run from a checkpoint and error. You can fix these errors by manually going into your google drive and deleting the checkpoint\n",
        "4.   GPT-2 XL (1558M parameter) model is unable to fine tune using this library. I would love it if it were otherwise, but its the sad truth that Colab Pro's High-RAM GPUs still don't have enough VRAM. I get the feeling that there's an easy way around this and that it's becuase of a bug, but investigating that will be for another day. For the time being select XL at your own peril.\n",
        "5.   The number of finetuning steps being 1000 has no real reference. We should probably double check to see if that is actually a sufficient amount of fine tuning.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZqs89sj7kXc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gpt_2_simple as gpt2\n",
        "import tensorflow as tf\n",
        "\n",
        "# Note trying to pretrain GPT-2 XL crashes even high-RAM colab pro\n",
        "GPT2_MODEL = \"774M\" # Small=124M, Medium=355M, Large=774M, XL=1558M\n",
        "FINETUNING_CORPUS = 'NYT' # The corpus to finetune on (\"NYT\", \"Reddit\", \"Dungeon\")\n",
        "FINETUNING_STEPS = 1\n",
        "\n",
        "if FINETUNING_CORPUS != SAMPLING_CORPUS:\n",
        "\tprint('Warning: You are finetuning on a different corpus than you sampled from!')\n",
        "\n",
        "if FINETUNING_CORPUS == 'Dungeon':\n",
        "  FINETUNING_FILE_NAME = './generation/AI-Dungeon/text_adventures_train.txt'\n",
        "elif FINETUNING_CORPUS == 'NYT':\n",
        "  FINETUNING_FILE_NAME = './generation/New-York-Times/nyt-articles-train.txt'\n",
        "else:\n",
        "  print('Error: Invalid sampling corpus name ' + FINETUNING_CORPUS)\n",
        "  exit(-1)\n",
        "\n",
        "if not os.path.isdir(os.path.join(\"models\", GPT2_MODEL)):\n",
        "\tprint(f\"Downloading {GPT2_MODEL} model...\")\n",
        "\tgpt2.download_gpt2(model_name=GPT2_MODEL)\n",
        "\n",
        "tf.reset_default_graph()\n",
        "sess = gpt2.start_tf_sess()\n",
        "gpt2.finetune(sess, FINETUNING_FILE_NAME, model_name=GPT2_MODEL, steps=FINETUNING_STEPS)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AP0infXDuK7b",
        "colab_type": "text"
      },
      "source": [
        "## Step 5: Generate Text with Fine-Tuned GPT-2 Model\n",
        "\n",
        "### Usage\n",
        "1.   Nothing special here, just run\n",
        "\n",
        "### Notes\n",
        "1.   We should probably split up the act of writing these generations to a JSON file every 20 or so generations. When I ran this for the first time, I crashed colab pro with OOM error at 94 generations and almost lost them all. This is probably the biggest TODO of the notebook in its current state.\n",
        "2.   We purposely re-roll the generation if GPT-2 ever gives us <|endoftext|> or <|startoftext|> (or in the case of New York Times <|startofarticle|> or <|endofarticle|>). This does mean that we're arbitrarily skewing the distribution but hopefully that doesn't effect our results too much\n",
        "3.   The newlines in both the prompt and generated text are a bit of a nuisance. Currently we do not explicitly do anything with the newlines present in the prompt and generation because nltk tokenization generally takes care of them. However this means we do have to join the tokenized sentences with newlines when feeding gpt-2 the prompt. We may want to change this to space.\n",
        "4.   We keep newlines untouched in an attempt to match the fine-tuning dataset as closely as possible and with the assumption that sometimes newlines are meaningful. However, in an ideal world, we would prefer to have sentences that do not span 10+ turns of dialogue (which we have actually seen, believe it or not). Maybe in the future we could solve this by replacing all newlines in the fine-tuning dataset with spaces. I wonder how much damage this would cause.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69ukY84r8bXL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if FINETUNING_CORPUS == 'Dungeon':\n",
        "  delimiter_tokens = ('<|startoftext|>', '<|endoftext|>')\n",
        "elif FINETUNING_CORPUS == 'NYT':\n",
        "  delimiter_tokens = ('<|startofarticle|>', '<|endofarticle|>')\n",
        "else:\n",
        "  print('Error: Invalid sampling corpus name ' + FINETUNING_CORPUS)\n",
        "  exit(-1)\n",
        "\n",
        "generations = []\n",
        "for index, prompt in enumerate(prompts):\n",
        "  if len(prompt) < MAX_PROMPT_LENGTH:\n",
        "    generation_is_good = False\n",
        "    while(not generation_is_good):\n",
        "      generated_text = gpt2.generate(sess, prefix='\\n'.join(prompt), return_as_list=True)[0]\n",
        "      generation_is_good = True\n",
        "      if any([token in generated_text for token in delimiter_tokens]):\n",
        "        generation_is_good = False\n",
        "\n",
        "    ## Create the final output by concatenating the generation to the prompt with the separating boundary token\n",
        "    final_generation = prompt + sent_tokenize(generated_text)[len(prompt):MAX_PROMPT_LENGTH]\n",
        "    boundary = len(prompt) - 1\n",
        "  else:\n",
        "    final_generation = prompt\n",
        "    boundary = -1\n",
        "\n",
        "  generation = {\n",
        "      'prompt': final_generation[0],\n",
        "      'text': final_generation[1:],\n",
        "      'boundary': boundary,\n",
        "  }\n",
        "\n",
        "  print('=============GENERATION NUMBER: ' + str(index) + '=============')\n",
        "  print(generation)\n",
        "  generations.append(generation)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKdlhROL4nh1",
        "colab_type": "text"
      },
      "source": [
        "## Step 6: Output to JSON\n",
        "\n",
        "### Notes:\n",
        "1.   Using JSON5 here becuase it handles double quotes within JSON fields well, which is super important for dialogue based prompts like the ones we see in text_adventures\n",
        "2.   We concat the UNIX timestamp into the output file name to prevent overwriting. It's up to you to later go into your drive and combine however many of these JSON files you want together. \n",
        "3.   We need to give these prompts a unique ID, something to connect the annotation to them permanently, maybe a combination of the timestamp plus their prompt number ?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlZn0YeoRBm2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip3 install json5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQeS4ASCBjVE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time, json5\n",
        "\n",
        "output_file = './generation/generations_' + str(int(time.time())) + '.json'\n",
        "\n",
        "with open(output_file, 'w+', encoding='utf-8') as f:\n",
        "    json5.dump(generations, f, ensure_ascii=False, indent=4)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}