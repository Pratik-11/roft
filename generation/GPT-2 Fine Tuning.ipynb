{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GPT-2 Fine Tuning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWeJ_SqyuPDw",
        "colab_type": "text"
      },
      "source": [
        "# Prompt Generation for ROFT (Real or Fake Text?) -- http://roft.io/\n",
        "\n",
        "Developed by Liam Dugan and Arun Kirubarajan in Spring 2020 ([Github](https://github.com/kirubarajan/roft.git))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZJ1zYjMvD-c",
        "colab_type": "text"
      },
      "source": [
        "## Step 1: Mount Drive and Clone the Repository"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9XpuJ-l7Z61H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Mount your google drive folder\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Change to the google drive folder and clone our repo and gpt-2\n",
        "import os\n",
        "os.chdir('/content/drive/My Drive')\n",
        "!git clone https://github.com/kirubarajan/roft.git\n",
        "os.chdir('/content/drive/My Drive/roft')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCj3zNwFvOlo",
        "colab_type": "text"
      },
      "source": [
        "## Step 2: Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jA9aInplD0E_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 1.x      # GPT-2 currently only supports tensorflow 1 \n",
        "!pip3 install gpt-2-simple\n",
        "import nltk \n",
        "nltk.download('punkt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHCUuYPMvh5I",
        "colab_type": "text"
      },
      "source": [
        "## Step 3: Sample Prompts from Human Text\n",
        "At the moment this is only from AI dungeon training data, but we will expand this to include news and maybe blog posts later. \n",
        "\n",
        "### Usage\n",
        "1.   Determine desired NUM_GENERATIONS, MAX_PROMPT_LENGTH, and PERCENT_NONHUMAN\n",
        "2.   Pick the desired SAMPLE_FILE\n",
        "3.   That's it! Run and see!\n",
        "\n",
        "### Notes\n",
        "1.   Prompt lengths are sampled on a *uniform* distribution (not a random distribution)\n",
        "2.   Prompts are only sampled from the AI Dungeon *test* file. We should probably concatenate both the test and dev file to get maximum prompt diversity.\n",
        "3.   The file is mmaped instead of loading it into RAM, so no need to worry about having a high-RAM instance of colab pro.\n",
        "4.   We only sample from the start of generations (<|startoftext|>). This allows us to avoid instances where a prompt contains context from a previous unseen part of the story\n",
        "5.   We use nltk punkt sentence tokenizer to tokenize sentences. This isn't really a perfect tokenizer to use, might be worth looking into other options\n",
        "6.   There ARE duplicates. This is because the input text_adventures_test.txt has duplicates. Will probably MD5 hash check at some point in the future to get rid of these\n",
        "7.   Prompts are always sampled from the beginning of the file and not randomly. This is to avoid running the regex on the entire file for large corpora. This can be changed in the future, but for now, do not expect this to give you two different sets of generations if run twice.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URFBnYZtbISX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os, re, mmap, random\n",
        "from nltk.tokenize import sent_tokenize"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4iRynfvRR-0S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def random_sample_prompt_length(percent_nonhuman, max_prompt_length):\n",
        "  if (random.random() > percent_nonhuman):\n",
        "    return max_prompt_length\n",
        "  else:\n",
        "    return random.randint(1, max_prompt_length - 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJub8y5VGmVc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sample_ai_dungeon(num_samples, max_prompt_length, percent_nonhuman):\n",
        "\n",
        "  # AI Dungeon sample file is text_adventures_test.txt\n",
        "  # (should probably update to include dev at some point)\n",
        "  sample_file = './generation/text_adventures_test.txt'\n",
        "  if not os.path.exists(sample_file):\n",
        "    print('Error: AI Dungeon sample file \"' + SAMPLE_FILE + '\" does not exist')\n",
        "    exit(-1)\n",
        "\n",
        "  prompts = []\n",
        "  successfully_sampled_prompts = 0\n",
        "  with open(sample_file, 'r+b') as f:\n",
        "    # mmap the file so we can regex it without loading it all into RAM\n",
        "    data = mmap.mmap(f.fileno(), 0)\n",
        "\n",
        "    # Grab all the spans of text that are between <|startoftext|> and <|endoftext|>\n",
        "    # (use finditer instead of findall to only search for regex matches as necessary)\n",
        "    pattern = re.compile(b'<\\|startoftext\\|\\>((.|\\n)*?)\\<\\|endoftext\\|\\>')\n",
        "    for m in re.finditer(pattern, data):\n",
        "      # If we're done sampling, no need to continue the loop\n",
        "      if successfully_sampled_prompts >= num_samples: break\n",
        "\n",
        "      # Randomly determine prompt length based on the specified percent nonhuman value\n",
        "      prompt_length = random_sample_prompt_length(percent_nonhuman, max_prompt_length)\n",
        "\n",
        "      # Use NLTK Sentence tokenizer to sample sentences and clean them\n",
        "      tokenized_prompt = sent_tokenize(str(m.group(1), 'utf-8', 'ignore'))\n",
        "\n",
        "      # Accept the prompt if it is longer than the desired length\n",
        "      if len(tokenized_prompt) > prompt_length:\n",
        "        prompts.append(tokenized_prompt[:prompt_length])\n",
        "        successfully_sampled_prompts += 1\n",
        "\n",
        "        print('Sampled prompt {0} of length {1}'.format(str(successfully_sampled_prompts), str(prompt_length)))\n",
        "        for line in tokenized_prompt[:prompt_length]:\n",
        "          print('\\t' + repr(line))\n",
        "\n",
        "  return prompts"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PS9ZUMJYNfB-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "import re\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "NUM_GENERATIONS = 56 # Number of prompts to sample\n",
        "MAX_PROMPT_LENGTH = 11 # This is the maximum length of the prompt (lengths will be uniformly sampled from 1 to this number)\n",
        "PERCENT_NONHUMAN = 0.75 # This is the percentage of the prompts to have be randomly sampled from length 1 to MAX, the rest will all be human only\n",
        "SAMPLE_FILE = './generation/text_adventures_test.txt' # file to sample prompts from\n",
        "\n",
        "prompts = sample_ai_dungeon(NUM_GENERATIONS, MAX_PROMPT_LENGTH, PERCENT_NONHUMAN)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEJb18g3tt27",
        "colab_type": "text"
      },
      "source": [
        "## Step 4: Download, Load, and Fine-Tune GPT-2\n",
        "\n",
        "### Usage\n",
        "1.   Determine which GPT2_MODEL_NAME to use (sizes are in comments)\n",
        "2.   Determine number of FINETUNING_STEPS (1000 steps on 774M w/ colab pro took about an hour for reference)\n",
        "3.   Determine PRETRAINING_FILE_NAME to pretrain on\n",
        "4.   Run and see!\n",
        "\n",
        "### Notes\n",
        "1.   The gpt_2_simple library has a bad habit of not working if you ever interrupt it, so try your best to not interrupt it pretraining. If you do you will likely have to restart the runtime.\n",
        "2.   One good side effect of using gpt_2_simple is that it implicitly saves and loads checkpoints. This means if you get disconnected at any point, you can restart and as long as you have the same drive mounted to the same folder with the same parameters, it will find your most recently fine-tuned model. \n",
        "3.   One bad side effect of using gpt_2_simple is that it implicitly saves and loads checkpoints. This means if you ever want to fine tune a different model or switch around some of your parameters, the library has a tendency to assume you want to run from a checkpoint and error. You can fix these errors by manually going into your google drive and deleting the checkpoint\n",
        "4.   GPT-2 XL (1558M parameter) model is unable to fine tune using this library. I would love it if it were otherwise, but its the sad truth that Colab Pro's High-RAM GPUs still don't have enough VRAM. I get the feeling that there's an easy way around this and that it's becuase of a bug, but investigating that will be for another day. For the time being select XL at your own peril.\n",
        "5.   The number of finetuning steps being 1000 has no real reference. We should probably double check to see if that is actually a sufficient amount of fine tuning.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZqs89sj7kXc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gpt_2_simple as gpt2\n",
        "\n",
        "# Note trying to pretrain GPT-2 XL crashes even high-RAM colab pro\n",
        "GPT2_MODEL_NAME = \"774M\" # Small = 124M, Medium = 355M, Large = 774M, XL = 1558M\n",
        "FINETUNING_STEPS = 1000\n",
        "FINETUNING_FILE_NAME = './samples/text_adventures_train.txt'\n",
        "\n",
        "if not os.path.isdir(os.path.join(\"models\", GPT2_MODEL_NAME)):\n",
        "\tprint(f\"Downloading {GPT2_MODEL_NAME} model...\")\n",
        "\tgpt2.download_gpt2(model_name=GPT2_MODEL_NAME)\n",
        "\n",
        "sess = gpt2.start_tf_sess()\n",
        "gpt2.finetune(sess, FINETUNING_FILE_NAME, model_name=model_name, steps=FINETUNING_STEPS)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AP0infXDuK7b",
        "colab_type": "text"
      },
      "source": [
        "## Step 5: Generate Text with Fine-Tuned GPT-2 Model\n",
        "\n",
        "### Usage\n",
        "1.   Nothing special here, just run\n",
        "\n",
        "### Notes\n",
        "1.   We should probably split up the act of writing these generations to a JSON file every 20 or so generations. When I ran this for the first time, I crashed colab pro with OOM error at 94 generations and almost lost them all. This is probably the biggest TODO of the notebook in its current state.\n",
        "2.   Once again like the prompt sampling, we purposely re-roll the generation if GPT-2 ever gives us <|endoftext|> or <|startoftext|>. This does mean that we're arbitrarily skewing the distribution but hopefully that doesn't effect our results too much\n",
        "3.   The newlines in both the prompt and generated text are a bit of a nuisance. Currently we do not explicitly do anything with the newlines present in the prompt and generation because nltk tokenization generally takes care of them. However this means we do have to join the tokenized sentences with newlines when feeding gpt-2 the prompt. We may want to change this to space.\n",
        "4.   We keep newlines untouched in an attempt to match the fine-tuning dataset as closely as possible and with the assumption that sometimes newlines are meaningful. However, in an ideal world, we would prefer to have sentences that do not span 10+ turns of dialogue (which we have actually seen, believe it or not). Maybe in the future we could solve this by replacing all newlines in the fine-tuning dataset with spaces. I wonder how much damage this would cause.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69ukY84r8bXL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json5\n",
        "\n",
        "generations = []\n",
        "for index, prompt in enumerate(prompts):\n",
        "  if len(prompt) < MAX_PROMPT_LENGTH:\n",
        "    generation_is_good = False\n",
        "    while(not generation_is_good):\n",
        "      generated_text = gpt2.generate(sess, prefix='\\n'.join(prompt), return_as_list=True)[0]\n",
        "      generation_is_good = True\n",
        "      if '<|startoftext|>' in generated_text or '<|endoftext|>' in generated_text:\n",
        "        generation_is_good = False\n",
        "\n",
        "    ## Create the final output by concatenating the generation to the prompt with the separating boundary token\n",
        "    final_generation = prompt + sent_tokenize(generated_text)[len(prompt):MAX_PROMPT_LENGTH]\n",
        "    boundary = len(prompt) - 1\n",
        "  else:\n",
        "    final_generation = prompt\n",
        "    boundary = -1\n",
        "\n",
        "  generation = {\n",
        "      'prompt': final_generation[0],\n",
        "      'text': final_generation[1:],\n",
        "      'boundary': boundary,\n",
        "  }\n",
        "\n",
        "  print('=============GENERATION NUMBER: ' + str(index) + '=============')\n",
        "  print(generation)\n",
        "  generations.append(generation)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKdlhROL4nh1",
        "colab_type": "text"
      },
      "source": [
        "## Step 6: Output to JSON\n",
        "\n",
        "### Notes:\n",
        "1.   Using JSON5 here becuase it handles double quotes within JSON fields well, which is super important for dialogue based prompts like the ones we see in text_adventures\n",
        "2.   We concat the UNIX timestamp into the output file name to prevent overwriting. It's up to you to later go into your drive and combine however many of these JSON files you want together. \n",
        "3.   We need to give these prompts a unique ID, something to connect the annotation to them permanently, maybe a combination of the timestamp plus their prompt number ?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQeS4ASCBjVE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "output_file = './samples/generations_' + str(int(time.time())) + '.json'\n",
        "\n",
        "with open(output_file, 'w', encoding='utf-8') as f:\n",
        "    json5.dump(generations, f, ensure_ascii=False, indent=4)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}